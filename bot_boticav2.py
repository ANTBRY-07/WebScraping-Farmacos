import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# --- CONFIGURACI√ìN ---
BASE_URL = "https://www.hogarysalud.com.pe/c/nutricion/"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'es-ES,es;q=0.9'
}

DATOS_RECOPILADOS = []

def obtener_sopa(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=20)
        if response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
    except Exception as e:
        print(f"‚ùå Error conectando a {url}: {e}")
    return None

def extraer_info_acordeon(url_producto):
    """
    Entra al producto y extrae TODAS las pesta√±as del acorde√≥n din√°micamente.
    Devuelve un diccionario (ej: {'Composici√≥n': '...', 'Advertencias': '...'})
    """
    soup = obtener_sopa(url_producto)
    info_extra = {} # Diccionario vac√≠o para guardar lo que encontremos
    
    if not soup: return info_extra
    
    # 1. Buscamos cada bloque del acorde√≥n
    items_acordeon = soup.select('div.wd-accordion-item')
    
    for item in items_acordeon:
        try:
            # 2. Extraer el T√≠tulo (Composici√≥n, Advertencias, etc.)
            titulo_tag = item.select_one('.wd-accordion-title-text')
            if not titulo_tag: continue
            
            titulo_texto = titulo_tag.get_text(strip=True)
            
            # 3. Extraer el Contenido (El texto oculto)
            contenido_tag = item.select_one('.woocommerce-Tabs-panel')
            
            if contenido_tag:
                # Usamos separator=' ' para que los p√°rrafos no se peguen
                contenido_texto = contenido_tag.get_text(separator=' ', strip=True)
                
                # Guardamos en el diccionario: Clave = T√≠tulo, Valor = Texto
                info_extra[titulo_texto] = contenido_texto
                
        except Exception as e:
            continue
            
    return info_extra

def escanear_catalogo():
    page = 1
    MAX_PAGES = 11 # Aumenta esto cuando quieras todo el cat√°logo
    
    while page <= MAX_PAGES:
        if page == 1: url = BASE_URL
        else: url = f"{BASE_URL}page/{page}/"
            
        print(f"\n--- üìÑ PROCESANDO P√ÅGINA {page} ---")
        soup = obtener_sopa(url)
        if not soup: break
        
        # Selector de productos (Confirmado que funciona)
        productos = soup.select('div.wd-product')
        if not productos: break
            
        print(f"üîç Encontrados {len(productos)} productos...")
        
        for prod in productos:
            try:
                # Datos b√°sicos
                tag_titulo = prod.select_one('.wd-entities-title a')
                if not tag_titulo: continue
                
                nombre = tag_titulo.get_text(strip=True)
                link = tag_titulo['href']
                
                tag_precio = prod.select_one('.price')
                # Limpieza de precio para que Excel lo entienda mejor
                precio = tag_precio.get_text(separator=' ', strip=True) if tag_precio else "0"
                
                # --- AQU√ç LA MAGIA: Extraer datos din√°micos ---
                # Entramos al link y traemos el diccionario con todo lo que haya
                diccionario_info = extraer_info_acordeon(link)
                
                # Creamos el objeto final fusionando datos b√°sicos + datos del acorde√≥n
                item_final = {
                    'Nombre': nombre,
                    'Precio': precio,
                    'URL': link
                }
                # Fusionamos el diccionario de info extra (Advertencias, Composici√≥n, etc.)
                item_final.update(diccionario_info)
                
                DATOS_RECOPILADOS.append(item_final)
                
                print(f"‚úÖ {nombre[:30]}... | Info extra√≠da: {list(diccionario_info.keys())}")
                
                time.sleep(random.uniform(0.5, 1.2)) # Pausa de cortes√≠a
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error: {e}")
                continue
        
        page += 1

# --- EJECUCI√ìN ---
print("üöÄ Iniciando Scraper Inteligente...")
escanear_catalogo()

# --- GUARDADO EN EXCEL (.xlsx) ---
if DATOS_RECOPILADOS:
    df = pd.DataFrame(DATOS_RECOPILADOS)
    
    # Esto guardar√° directamente en formato Excel con celdas separadas
    nombre_archivo = 'catalogo_completo_dinamico.xlsx'
    df.to_excel(nombre_archivo, index=False)
    
    print(f"\nüéâ ¬°√âXITO! Se gener√≥ '{nombre_archivo}'.")
    print("Nota: Las columnas se crearon autom√°ticamente seg√∫n la info encontrada.")
else:
    print("\nNo se encontraron datos.")