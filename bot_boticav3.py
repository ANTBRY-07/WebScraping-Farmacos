import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import unicodedata
import re
import concurrent.futures  # <--- Librer√≠a para ejecuci√≥n paralela (Multithreading)

# ==============================================================================
# CONFIGURACI√ìN GENERAL DEL SCRIPT
# ==============================================================================

# Nombre del archivo de texto que contiene la lista de medicamentos del MINSA
NOMBRE_ARCHIVO_LISTA = "lista_minsa.txt"

# URL base de la farmacia a scrapear
URL_HOME = "https://www.hogarysalud.com.pe"

# Configuraci√≥n del motor de paralelismo
# NOTA: Mantener entre 5 y 10 workers. Un n√∫mero mayor podr√≠a causar
# que el servidor bloquee tu direcci√≥n IP por "Denegaci√≥n de Servicio" (DoS).
MAX_WORKERS = 5

# Cabeceras HTTP para simular un navegador real y evitar bloqueos b√°sicos
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'es-ES,es;q=0.9'
}

# Variables globales para almacenamiento
DATOS_RECOPILADOS = []
LISTA_MINSA = set()

# Configuraci√≥n de la sesi√≥n HTTP
# Usar 'Session' permite reutilizar la conexi√≥n TCP (Keep-Alive),
# lo que acelera significativamente las peticiones m√∫ltiples.
session = requests.Session()
session.headers.update(HEADERS)


# ==============================================================================
# 1. BLOQUE DE HERRAMIENTAS Y FILTROS DE TEXTO
# ==============================================================================

def normalizar(texto):
    """
    Elimina tildes y convierte el texto a may√∫sculas para facilitar comparaciones.
    Ejemplo: '√Åcido' -> 'ACIDO'
    
    Args:
        texto (str): El texto original.
        
    Returns:
        str: Texto limpio y normalizado.
    """
    if not isinstance(texto, str):
        return ""
    
    texto = texto.upper()
    
    # Normalizaci√≥n NFD para separar caracteres base de sus acentos
    return ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')


def cargar_filtro_txt():
    """
    Lee el archivo de texto l√≠nea por l√≠nea y carga los medicamentos en memoria (Set).
    Se usa un 'Set' (conjunto) porque la b√∫squeda es mucho m√°s r√°pida que en una lista.
    """
    print(f"üìñ Leyendo lista segura: {NOMBRE_ARCHIVO_LISTA}...")
    global LISTA_MINSA
    
    try:
        with open(NOMBRE_ARCHIVO_LISTA, 'r', encoding='utf-8') as f:
            for linea in f:
                med = linea.strip()
                # Solo guardamos si tiene una longitud m√≠nima para evitar ruido
                if len(med) > 3:
                    LISTA_MINSA.add(normalizar(med))
                    
        print(f"‚úÖ Filtro cargado: {len(LISTA_MINSA)} medicamentos listos para filtrar.")
        
    except FileNotFoundError:
        print(f"‚ùå ERROR CR√çTICO: No se encontr√≥ el archivo '{NOMBRE_ARCHIVO_LISTA}'. Crea el archivo antes de continuar.")


def cumple_filtro_minsa(nombre_producto_web):
    """
    Verifica si el nombre del producto encontrado en la web contiene
    alguna de las palabras clave de la lista del MINSA.
    
    Args:
        nombre_producto_web (str): Nombre extra√≠do de la web.
        
    Returns:
        bool: True si es un medicamento esencial, False si no lo es.
    """
    nombre_norm = normalizar(nombre_producto_web)
    
    for med in LISTA_MINSA:
        # Verificamos coincidencia exacta de palabra o inicio de frase
        # para evitar falsos positivos (ej: que "AJO" active "BAJO").
        if f" {med} " in f" {nombre_norm} " or nombre_norm.startswith(f"{med} ") or med == nombre_norm:
            return True
            
    return False


def analizar_precios(texto_precio):
    """
    Extrae los valores num√©ricos de una cadena de texto de precio.
    Maneja rangos de precios (ej: "S/ 10.00 - S/ 20.00").
    
    Args:
        texto_precio (str): Texto bruto del precio (ej: "S/ 12.50").
        
    Returns:
        tuple: (precio_minimo, precio_maximo) como flotantes.
    """
    if not texto_precio:
        return 0.0, 0.0
    
    # Regex para encontrar n√∫meros con formato decimal (ej: 10.50)
    numeros = re.findall(r'(\d+\.\d{2})', texto_precio)
    
    if not numeros:
        return 0.0, 0.0
    
    # Convertimos strings a floats
    valores = [float(n) for n in numeros]
    
    # Devolvemos el m√≠nimo y m√°ximo encontrado
    return min(valores), max(valores)


# ==============================================================================
# 2. BLOQUE DE NAVEGACI√ìN WEB (CRAWLER)
# ==============================================================================

def obtener_sopa(url):
    """
    Realiza una petici√≥n GET a la URL y devuelve el objeto BeautifulSoup.
    Maneja excepciones silenciosamente para no detener el flujo masivo.
    
    Args:
        url (str): Direcci√≥n web a consultar.
        
    Returns:
        BeautifulSoup object | None: El HTML parseado o None si fall√≥.
    """
    try:
        # Usamos 'session' para mantener cookies y conexi√≥n viva
        response = session.get(url, timeout=20)
        
        if response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
            
    except Exception:
        # En scraping masivo, a veces es mejor ignorar errores puntuales
        pass
        
    return None


def descubrir_categorias_menu():
    """
    Escanea la p√°gina principal para encontrar los enlaces de las categor√≠as.
    Se enfoca en el men√∫ espec√≠fico 'menu-mega-menu-categorias' para evitar enlaces basura.
    
    Returns:
        list: Lista de URLs de categor√≠as.
    """
    print(f"üåé Conectando a {URL_HOME} para descubrir cat√°logo...")
    
    soup = obtener_sopa(URL_HOME)
    if not soup:
        return []
    
    lista_links = []
    
    # Buscamos el contenedor espec√≠fico del men√∫
    menu = soup.find('ul', id='menu-mega-menu-categorias')
    
    if menu:
        # 'recursive=False' asegura que solo tomamos las categor√≠as principales
        # y no sub-niveles que podr√≠an duplicar la b√∫squeda.
        for item in menu.find_all('li', recursive=False):
            link = item.find('a', href=True)
            
            # Filtro adicional para asegurar que es un link de categor√≠a v√°lida
            if link and '/c/' in link['href']:
                lista_links.append(link['href'])
                
    return lista_links


# ==============================================================================
# 3. L√ìGICA DE PROCESAMIENTO PARALELO (WORKER)
# ==============================================================================

def procesar_producto_individual(datos_base):
    """
    FUNCI√ìN PRINCIPAL DEL HILO (WORKER).
    Esta funci√≥n se ejecuta de forma paralela para m√∫ltiples productos a la vez.
    Realiza el filtrado y, si pasa, entra al detalle del producto (Deep Scraping).
    
    Args:
        datos_base (dict): Diccionario con Nombre, URL y Precio inicial.
        
    Returns:
        dict | None: Datos completos del producto o None si fue filtrado/fall√≥.
    """
    try:
        nombre = datos_base['Nombre']
        link = datos_base['URL']
        
        # --- PASO 1: FILTRADO R√ÅPIDO ---
        # Verificamos si el producto est√° en la lista del MINSA antes de hacer
        # la petici√≥n web, para ahorrar tiempo y recursos.
        if not cumple_filtro_minsa(nombre):
            return None # Se descarta el producto
            
        # --- PASO 2: EXTRACCI√ìN PROFUNDA (DEEP SCRAPING) ---
        # Pausa aleatoria para simular comportamiento humano y evitar bloqueos
        time.sleep(random.uniform(0.1, 0.5)) 
        
        soup = obtener_sopa(link)
        
        # Diccionario por defecto para campos opcionales
        info_adicional = {
            'Registro Sanitario': 'No especificado',
            'Composici√≥n': 'No especificado',
            'Descripci√≥n': 'No especificado',
            'Advertencias': 'No especificado',
            'Contraindicaciones': 'No especificado'
        }
        
        if soup:
            # A. Buscar informaci√≥n en los acordeones (pesta√±as desplegables)
            # ------------------------------------------------------------
            for item in soup.select('div.wd-accordion-item'):
                t_tag = item.select_one('.wd-accordion-title-text')
                c_tag = item.select_one('.woocommerce-Tabs-panel')
                
                if t_tag and c_tag:
                    titulo = t_tag.get_text(strip=True).lower()
                    contenido = c_tag.get_text(separator=' ', strip=True)
                    
                    # Asignaci√≥n din√°mica seg√∫n palabras clave en el t√≠tulo
                    if 'descripci' in titulo:
                        info_adicional['Descripci√≥n'] = contenido
                    elif 'advertencia' in titulo:
                        info_adicional['Advertencias'] = contenido
                    elif 'contraindicaci' in titulo:
                        info_adicional['Contraindicaciones'] = contenido
                    elif 'composici' in titulo:
                        info_adicional['Composici√≥n'] = contenido
            
            # B. Buscar informaci√≥n en la tabla de atributos t√©cnicos
            # ------------------------------------------------------------
            for row in soup.select('tr.woocommerce-product-attributes-item'):
                th = row.select_one('th')
                td = row.select_one('td')
                
                if th and td:
                    label = th.get_text(strip=True).lower()
                    valor = td.get_text(strip=True)
                    
                    if 'registro' in label:
                        info_adicional['Registro Sanitario'] = valor
                    elif 'composici' in label and info_adicional['Composici√≥n'] == 'No especificado':
                        info_adicional['Composici√≥n'] = valor

        # Fusionamos los datos base con la informaci√≥n extra√≠da
        datos_base.update(info_adicional)
        
        return datos_base

    except Exception as e:
        # Si algo falla dentro del hilo, retornamos None para no romper el proceso
        return None


# ==============================================================================
# 4. GESTOR DE CATEGOR√çA Y PAGINACI√ìN (MANAGER)
# ==============================================================================

def procesar_categoria(url_cat):
    """
    Controla la paginaci√≥n de una categor√≠a y distribuye los productos
    encontrados a los 'Workers' para su procesamiento en paralelo.
    
    Args:
        url_cat (str): URL de la categor√≠a a procesar.
    """
    page = 1
    MAX_PAGES = 100 # L√≠mite de seguridad
    
    # Extraemos el nombre limpio de la categor√≠a desde la URL
    nombre_cat = url_cat.strip('/').split('/')[-1].replace('-', ' ').title()
    print(f"\nüìÇ PROCESANDO CATEGOR√çA: {nombre_cat}")

    while page <= MAX_PAGES:
        # Construcci√≥n de la URL paginada
        url_actual = url_cat if page == 1 else f"{url_cat}page/{page}/"
        
        soup = obtener_sopa(url_actual)
        if not soup:
            break
        
        # Selector para encontrar las "cajas" de los productos
        productos_html = soup.select('div.wd-product')
        
        if not productos_html:
            print("   -> No se encontraron m√°s productos. Fin de categor√≠a.")
            break
        
        print(f"  --> P√°g {page}: {len(productos_html)} productos detectados. Iniciando an√°lisis paralelo...")
        
        # --- FASE A: PREPARACI√ìN DE TAREAS ---
        # Recopilamos la info b√°sica de cada producto en la rejilla
        tareas_para_workers = []
        
        for prod in productos_html:
            tag_a = prod.select_one('.wd-entities-title a')
            if not tag_a: continue
            
            # Extracci√≥n y limpieza de precios
            tag_p = prod.select_one('.price')
            txt_p = tag_p.get_text(separator=' ', strip=True) if tag_p else ""
            p_min, p_max = analizar_precios(txt_p)
            
            datos_iniciales = {
                'Categor√≠a': nombre_cat,
                'Nombre': tag_a.get_text(strip=True),
                'Precio M√≠nimo (S/)': p_min,
                'Precio M√°ximo (S/)': p_max,
                'URL': tag_a['href']
            }
            tareas_para_workers.append(datos_iniciales)

        # --- FASE B: EJECUCI√ìN PARALELA (ThreadPoolExecutor) ---
        # Aqu√≠ es donde ocurre la magia de la velocidad. Se lanzan m√∫ltiples hilos.
        guardados_pagina = 0
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Mapeamos la funci√≥n 'procesar_producto_individual' a cada tarea
            resultados = list(executor.map(procesar_producto_individual, tareas_para_workers))
        
        # --- FASE C: RECOLECCI√ìN DE RESULTADOS ---
        # Filtramos los 'None' (productos que no pasaron el filtro o fallaron)
        for res in resultados:
            if res: 
                DATOS_RECOPILADOS.append(res)
                guardados_pagina += 1
        
        print(f"      ‚úÖ Se guardaron {guardados_pagina} productos esenciales de esta p√°gina.")
        
        # Verificar si existe bot√≥n de 'Siguiente p√°gina'
        if not soup.select_one('.next'):
            break
            
        page += 1


# ==============================================================================
# 5. BLOQUE DE EJECUCI√ìN PRINCIPAL
# ==============================================================================

if __name__ == "__main__":
    # 1. Cargar base de datos del MINSA
    cargar_filtro_txt()
    
    if LISTA_MINSA:
        # 2. Obtener categor√≠as de la web
        cats = descubrir_categorias_menu()
        
        if cats:
            print(f"\nüöÄ INICIANDO SCRAPING MASIVO CON {MAX_WORKERS} HILOS...")
            start_time = time.time() # Iniciar cron√≥metro
            
            # 3. Procesar cada categor√≠a encontrada
            for cat in cats:
                procesar_categoria(cat)
            
            # 4. Guardar resultados
            if DATOS_RECOPILADOS:
                print("\nüíæ Guardando datos en Excel...")
                df = pd.DataFrame(DATOS_RECOPILADOS)
                
                nombre_archivo = 'catalogo_turbo_minsa.xlsx'
                df.to_excel(nombre_archivo, index=False)
                
                # C√°lculo de tiempo total
                mins = (time.time() - start_time) / 60
                print(f"\nüèÅ ¬°PROCESO TERMINADO EN {mins:.2f} MINUTOS!")
                print(f"üìÑ Archivo generado: {nombre_archivo}")
                print(f"üì¶ Total productos: {len(DATOS_RECOPILADOS)}")
                
            else:
                print("\n‚ö†Ô∏è El script finaliz√≥ pero no encontr√≥ coincidencias con la lista del MINSA.")
        else:
            print("\n‚ùå No se pudieron detectar categor√≠as en la p√°gina web.")
    else:
        print("\n‚ùå La lista del MINSA est√° vac√≠a o no se pudo cargar.")