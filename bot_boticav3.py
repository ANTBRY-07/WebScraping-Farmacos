import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import unicodedata

# --- CONFIGURACI√ìN ---
NOMBRE_ARCHIVO_LISTA = "lista_minsa.txt"
URL_HOME = "https://www.hogarysalud.com.pe"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'es-ES,es;q=0.9'
}

DATOS_RECOPILADOS = []
LISTA_MINSA = set()

# ==========================================
# 1. HERRAMIENTAS DE TEXTO (Normalizaci√≥n)
# ==========================================
def normalizar(texto):
    """ Quita tildes, pasa a may√∫sculas y limpia espacios """
    if not isinstance(texto, str): return ""
    texto = texto.upper()
    return ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')

# ==========================================
# 2. CARGADOR DE LISTA (Filtro TXT)
# ==========================================
def cargar_filtro_txt():
    print(f"üìñ Leyendo lista de medicamentos esenciales desde: {NOMBRE_ARCHIVO_LISTA}...")
    global LISTA_MINSA
    try:
        with open(NOMBRE_ARCHIVO_LISTA, 'r', encoding='utf-8') as f:
            for linea in f:
                med = linea.strip()
                if len(med) > 3:
                    LISTA_MINSA.add(normalizar(med))
        print(f"‚úÖ Filtro cargado: {len(LISTA_MINSA)} medicamentos seguros.")
    except FileNotFoundError:
        print(f"‚ùå ERROR: No existe '{NOMBRE_ARCHIVO_LISTA}'. Crea el archivo primero.")

def cumple_filtro_minsa(nombre_producto_web):
    """ Compara el producto web contra tu lista TXT normalizada """
    nombre_norm = normalizar(nombre_producto_web)
    for med_clave in LISTA_MINSA:
        # L√≥gica de coincidencia robusta (evita falsos positivos parciales)
        if f" {med_clave} " in f" {nombre_norm} " or \
           nombre_norm.startswith(f"{med_clave} ") or \
           med_clave == nombre_norm:
            return True
    return False

# ==========================================
# 3. EL NAVEGADOR (Crawler Mejorado)
# ==========================================
def obtener_sopa(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=25)
        if response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
    except Exception as e:
        print(f"‚ö†Ô∏è Error conectando a {url}: {e}")
    return None

def descubrir_categorias_menu():
    """ Usa la l√≥gica del primer c√≥digo que funcionaba mejor para detectar el men√∫ principal """
    print(f"üåé Conectando a {URL_HOME} para leer el men√∫...")
    soup = obtener_sopa(URL_HOME)
    
    if not soup: return []

    lista_urls_final = []
    
    # Buscamos el contenedor del men√∫ por su ID espec√≠fico
    menu_container = soup.find('ul', id='menu-mega-menu-categorias')
    
    if menu_container:
        print("‚úÖ Men√∫ encontrado. Filtrando solo categor√≠as principales...")
        # recursive=False es clave para no tomar subcategor√≠as
        items_principales = menu_container.find_all('li', recursive=False)
        
        for item in items_principales:
            enlace = item.find('a', href=True)
            if enlace:
                url = enlace['href']
                texto = enlace.get_text(strip=True)
                
                # Validaci√≥n de seguridad
                if '/c/' in url and 'hogarysalud.com.pe' in url:
                    lista_urls_final.append(url)
                    print(f"   üîπ Categor√≠a detectada: {texto}")
    else:
        print("‚ö†Ô∏è No se pudo leer el men√∫ principal (ID no encontrado).")

    print(f"üìã Total: {len(lista_urls_final)} categor√≠as listas para procesar.")
    return lista_urls_final

# ==========================================
# 4. EXTRACTOR DE DETALLES (Deep Scraping)
# ==========================================
def extraer_detalles_profundos(soup_producto):
    """ Extrae Registro Sanitario, Composici√≥n, etc. del detalle del producto """
    info = {
        'Descripci√≥n': 'No especificado',
        'Advertencias': 'No especificado',
        'Contraindicaciones': 'No especificado',
        'Composici√≥n': 'No especificado',
        'Registro Sanitario': 'No especificado'
    }
    
    if not soup_producto: return info

    # A) BUSCAR EN ACORDEONES (Pesta√±as desplegables)
    items_acordeon = soup_producto.select('div.wd-accordion-item')
    for item in items_acordeon:
        try:
            titulo_tag = item.select_one('.wd-accordion-title-text')
            contenido_tag = item.select_one('.woocommerce-Tabs-panel')
            
            if titulo_tag and contenido_tag:
                titulo = titulo_tag.get_text(strip=True).lower()
                contenido = contenido_tag.get_text(separator=' ', strip=True)
                
                if 'descripci' in titulo: info['Descripci√≥n'] = contenido
                elif 'advertencia' in titulo: info['Advertencias'] = contenido
                elif 'contraindicaci' in titulo: info['Contraindicaciones'] = contenido
                elif 'composici' in titulo: info['Composici√≥n'] = contenido
        except: continue

    # B) BUSCAR EN TABLA DE ATRIBUTOS (Registro Sanitario)
    tabla_atributos = soup_producto.select('tr.woocommerce-product-attributes-item')
    for fila in tabla_atributos:
        try:
            texto_label = fila.select_one('th').get_text(strip=True).lower()
            texto_valor = fila.select_one('td').get_text(strip=True)
            
            if 'registro' in texto_label or 'sanitario' in texto_label:
                info['Registro Sanitario'] = texto_valor
            elif 'composici' in texto_label and info['Composici√≥n'] == 'No especificado':
                info['Composici√≥n'] = texto_valor
        except: continue

    return info

# ==========================================
# 5. PROCESADOR PRINCIPAL (Mejorado)
# ==========================================
def procesar_categoria(url_categoria):
    page = 1
    MAX_PAGES = 100 # Ajusta seg√∫n necesites
    
    # Obtener nombre limpio de la categor√≠a
    nombre_cat = url_categoria.strip('/').split('/')[-1].replace('-', ' ').title()
    print(f"\nüìÇ PROCESANDO: {nombre_cat}")
    
    while page <= MAX_PAGES:
        url_actual = url_categoria if page == 1 else f"{url_categoria}page/{page}/"
        soup = obtener_sopa(url_actual)
        
        if not soup: break
        
        # Selector de productos
        productos = soup.select('div.wd-product')
        if not productos: break
            
        print(f"  --> P√°g {page}: {len(productos)} productos detectados.")
        
        contador_guardados = 0
        for prod in productos:
            try:
                # 1. Extracci√≥n B√°sica
                tag_titulo = prod.select_one('.wd-entities-title a')
                if not tag_titulo: continue
                
                nombre = tag_titulo.get_text(strip=True)
                
                # 2. FILTRO MINSA (Solo procesamos si es esencial)
                if not cumple_filtro_minsa(nombre):
                    continue 
                
                link = tag_titulo['href']
                tag_precio = prod.select_one('.price')
                precio = tag_precio.get_text(separator=' ', strip=True) if tag_precio else "0"
                
                # 3. EXTRACCI√ìN PROFUNDA (Entrar al link)
                time.sleep(random.uniform(0.5, 1.0)) # Pausa antibloqueo
                soup_detalle = obtener_sopa(link)
                detalles = extraer_detalles_profundos(soup_detalle)
                
                # 4. Consolidaci√≥n de datos
                item_final = {
                    'Categor√≠a': nombre_cat,
                    'Nombre': nombre,
                    'Precio': precio,
                    'Registro Sanitario': detalles['Registro Sanitario'],
                    'Composici√≥n': detalles['Composici√≥n'],
                    'Descripci√≥n': detalles['Descripci√≥n'],
                    'Advertencias': detalles['Advertencias'],
                    'Contraindicaciones': detalles['Contraindicaciones'],
                    'URL': link
                }
                
                DATOS_RECOPILADOS.append(item_final)
                contador_guardados += 1
                
            except Exception: continue
        
        print(f"      ‚úÖ Guardados: {contador_guardados} productos esenciales.")
        
        # Paginaci√≥n
        if not soup.select_one('.next'): break
        page += 1

# ==========================================
# EJECUCI√ìN MAESTRA
# ==========================================
# 1. Cargamos el filtro TXT
cargar_filtro_txt()

if LISTA_MINSA:
    # 2. Obtenemos las categor√≠as principales
    cats = descubrir_categorias_menu()
    
    if cats:
        print(f"üöÄ Iniciando extracci√≥n filtrada en {len(cats)} categor√≠as...")
        for cat in cats:
            procesar_categoria(cat)
            time.sleep(2) # Respiro entre categor√≠as
        
        # 3. Guardado final
        if DATOS_RECOPILADOS:
            df = pd.DataFrame(DATOS_RECOPILADOS)
            archivo_final = 'catalogo_minsa_completo.xlsx'
            df.to_excel(archivo_final, index=False)
            print(f"\nüéâ ¬°√âXITO! Se ha generado: {archivo_final}")
            print(f"Total de productos procesados: {len(DATOS_RECOPILADOS)}")
        else:
            print("\n‚ö†Ô∏è El script termin√≥, pero no encontr√≥ coincidencias con tu lista del MINSA.")
    else:
        print("‚ùå No se encontraron categor√≠as en la web.")
else:
    print("‚ùå Lista MINSA vac√≠a o archivo no encontrado.")